{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pseudolikelihood (finally)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm using the same convention as conIII -- we use upper triangular matrices, the first N elements are the local fields (we set them to 0), the other N(N-1)/2 are the elements of the matrix, going left to right. This is an important convention because otherwise calc_corr_obs is only compatible with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from scipy.spatial.distance import squareform\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from clean_data import voting_data as samples\n",
    "samples = np.array(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_heatmap(J):\n",
    "    J_ = J.reshape((9,9))\n",
    "    cm = sns.diverging_palette(248, 12, sep=1, n=256)\n",
    "    sns.heatmap(J_, cmap=cm, vmin=-1, vmax=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_obs_r(r, samples):\n",
    "    \"\"\" This functions populates a matrix with the same shape as the samples (num_samples x num_\n",
    "    dimensions) with the value s_r * s_j for every data sample. s_j is in order from least to \n",
    "    greatest. \"\"\"\n",
    "    N = len(samples[0])\n",
    "    \n",
    "    obs = np.zeros(np.shape(samples))\n",
    "    \n",
    "    \n",
    "    for rowix in range(len(samples)):\n",
    "        obs[rowix, 0] = samples[rowix, r]\n",
    "        ixcount = 1\n",
    "        for i in range(N - 1):\n",
    "            for j in range(i + 1, N):\n",
    "                if i == r or j == r:\n",
    "                    obs[rowix, ixcount] = samples[rowix, i] * samples[rowix, j]\n",
    "                    ixcount += 1\n",
    "    return(obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def utr_idx(N, i, j):\n",
    "    \"\"\" Do not question this function. It converts square indices to flat k=1 upper triangular\n",
    "    indices. \"\"\"\n",
    "    if i == j: raise(ValueError(\"i and j cannot be equal\"))\n",
    "    elif i < j: return (2 * N + 1-i) * i//2 + j - i - (i +1)\n",
    "    else: return utr_idx(N, j, i)\n",
    "\n",
    "def calc_jij_r(r, J, N):\n",
    "    \"\"\" Returns a vector of J_rj from an upper triangular k = 1 matrix of J. The upper triangular k = 1\n",
    "    matrix is equal to np.concatenate([np.diag(J), Jsquare[np.triu(len(Jsquare), k=1)]]). Choosing this\n",
    "    so I can compare it to conIII if need be. \"\"\"\n",
    "    if len(J) != N*(N+1) / 2: raise ValueError(\"J should be a vector of concatenated local fields and\" +\n",
    "                                               \" upper triangular elements.\")\n",
    "    all_other_elements = [i for i in range(N) if i != r]\n",
    "    r_ind = np.sort([utr_idx(N, r, j) for j in all_other_elements]) + N\n",
    "    jr_ind = np.concatenate(([r],r_ind))\n",
    "    return([J[jr_ind], jr_ind])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minimizer_func(params, samples):\n",
    "    \"\"\" Params is the normal concatenation of h and Jij \"\"\"\n",
    "    loglikelihood = 0\n",
    "    N = len(samples[0])\n",
    "    dloglikelihood = np.zeros_like(params)\n",
    "    \n",
    "    for r in range(N):\n",
    "        obs = calc_obs_r(r, samples)\n",
    "        jr_vector, jr_ind = calc_jij_r(r, params, N)\n",
    "        E = -obs.dot(jr_vector)\n",
    "        loglikelihood += -np.log(1 + np.exp(2 * E)).sum()\n",
    "        dloglikelihood[jr_ind] += ( -(1/(1+np.exp(2*E)) * np.exp(2*E))[:,None] * 2*obs ).sum(0)\n",
    "    return(-loglikelihood, dloglikelihood)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_for_jij(samples):\n",
    "    N = len(samples[0])\n",
    "    initial_guess = np.zeros(int(N * (N+1) / 2))\n",
    "    soln = minimize(minimizer_func, initial_guess, jac=True, args=(samples))\n",
    "    return(soln.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mysol = solve_for_jij(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_heatmap(squareform(mysol[9:]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
